# -*- coding: utf-8 -*-
"""
train_tabular_baseline_full.py
- åˆä½µ data/meta.csv èˆ‡ data/meta/meta_*.csv
- åªç•™ understood / confused
- ç‰¹å¾µï¼šå„ªå…ˆç”¨æŒ‡å®šæ¸…å–®ï¼Œä¸åœ¨å°±è‡ªå‹•ç”¨æ•¸å€¼æ¬„
- ä»¥ subject åˆ†å‰²ï¼›è‹¥åªæœ‰ 1 ä½å—æ¸¬è€… â†’ é€€å›æ¨£æœ¬åˆ†å‰²ï¼ˆç›¡é‡ stratifiedï¼‰
- ä¿éšœå…©é‚Šéƒ½å«å…©å€‹é¡åˆ¥ï¼ˆè‹¥è¾¦å¾—åˆ°ï¼‰ï¼Œå¿…è¦æ™‚è‡ªå‹•æ› seed / èª¿æ•´ test_size
- StandardScaler + LogisticRegression(class_weight='balanced')
- åˆ—å°è©³ç´°çµ±è¨ˆï¼Œå­˜æ¨¡å‹èˆ‡è¨“ç·´æ‘˜è¦ JSON
"""

from pathlib import Path
import pandas as pd
import numpy as np
import json, joblib, argparse

from sklearn.model_selection import GroupShuffleSplit, StratifiedShuffleSplit, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, f1_score

PREFERRED_FEATS = [
    "Aw","Ca","Hp","B","M","magnitude","direction",
    "ear_l","ear_r","mar","ipd","face_area"
]

def load_all_meta(data_dir: Path) -> pd.DataFrame:
    meta_dir = data_dir / "meta"
    dfs = []
    if meta_dir.exists():
        for p in sorted(meta_dir.glob("meta_*.csv")):
            try:
                dfs.append(pd.read_csv(p))
            except Exception as e:
                print(f"âš ï¸ è®€å–å¤±æ•—ï¼ˆç•¥éï¼‰{p}: {e}")
    legacy = data_dir / "meta.csv"
    if legacy.exists():
        dfs.append(pd.read_csv(legacy))
    if not dfs:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ä»»ä½• CSVï¼š{meta_dir/'meta_*.csv'} æˆ– {legacy}")
    return pd.concat(dfs, ignore_index=True)

def pick_features(df: pd.DataFrame) -> list:
    feats = [c for c in PREFERRED_FEATS if c in df.columns]
    if feats:
        return feats
    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    for bad in ["subject"]:
        if bad in num_cols:
            num_cols.remove(bad)
    return num_cols

def derive_groups(df: pd.DataFrame) -> np.ndarray:
    if "subject" in df.columns:
        return df["subject"].astype(str).values
    if "img" in df.columns:
        return df["img"].astype(str).apply(
            lambda s: Path(s).stem.split("_")[0] if "_" in Path(s).stem else "unknown"
        ).values
    return np.array(["unknown"] * len(df))

def ensure_two_classes(y_tr, y_va, max_tries=0):
    """å›å‚³ True è¡¨ç¤ºå…©é‚Šéƒ½æœ‰å…©å€‹é¡åˆ¥ï¼›max_tries ä¿ç•™ä»‹é¢ä½†æ­¤å‡½å¼åªåšæª¢æŸ¥"""
    ok_tr = len(np.unique(y_tr)) >= 2
    ok_va = len(np.unique(y_va)) >= 2
    return ok_tr and ok_va

def robust_split(X, y, groups, test_size=0.2, base_seed=42):
    uniq_groups = np.unique(groups.astype(str))
    n_groups = len(uniq_groups)
    n_samples = len(y)

    if n_samples < 2:
        raise ValueError("æ¨£æœ¬æ•¸å¤ªå°‘ï¼Œè‡³å°‘éœ€è¦ 2 ç­†ã€‚")

    # ç›¡é‡è®“ train/val å…©é‚Šéƒ½åŒ…å«å…©å€‹é¡åˆ¥
    seeds = [base_seed + i for i in range(50)]
    sizes = [test_size] + [max(0.1, min(0.4, test_size + d)) for d in (-0.05, 0.05, -0.1, 0.1)]

    if n_groups >= 2:
        print(f"[Split] ä»¥ subject åˆ†å‰²ï¼ˆ{n_groups} äººï¼‰")
        for s in seeds:
            gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=s)
            tr_idx, va_idx = next(gss.split(X, y, groups))
            if ensure_two_classes(y[tr_idx], y[va_idx]):
                print(f"  âœ“ ä½¿ç”¨ seed={s}")
                return tr_idx, va_idx
        # é€€ä¸€æ­¥ï¼šæ¥å—ç•¶å‰åˆ‡æ³•ï¼ˆå¯èƒ½ä¸€é‚Šåªæœ‰å–®é¡åˆ¥ï¼‰
        print("  âš ï¸ ç„¡æ³•åŒæ™‚ä¿è­‰å…©é‚Šçš†å«å…©é¡åˆ¥ï¼Œæ¡ç”¨æœ€å¾Œä¸€æ¬¡åˆ‡åˆ†")
        return tr_idx, va_idx

    # åªæœ‰ä¸€ä½å—æ¸¬è€… â†’ æ¨£æœ¬åˆ†å‰²
    print("[Split] åªæœ‰ 1 ä½å—æ¸¬è€… â†’ æ”¹ç”¨æ¨£æœ¬åˆ†å‰²")
    classes, counts = np.unique(y, return_counts=True)
    can_strat = (len(classes) > 1) and all(c >= 2 for c in counts)

    for s in seeds:
        for ts in sizes:
            if can_strat:
                sss = StratifiedShuffleSplit(n_splits=1, test_size=ts, random_state=s)
                tr_idx, va_idx = next(sss.split(X, y))
            else:
                tr_idx, va_idx = train_test_split(
                    np.arange(n_samples), test_size=ts, random_state=s, shuffle=True
                )
            if ensure_two_classes(y[tr_idx], y[va_idx]) or not can_strat:
                print(f"  âœ“ ä½¿ç”¨ seed={s}, test_size={ts}")
                return tr_idx, va_idx

    print("  âš ï¸ ç„¡æ³•åŒæ™‚ä¿è­‰å…©é‚Šçš†å«å…©é¡åˆ¥ï¼ˆè³‡æ–™é‡/åˆ†ä½ˆä¸è¶³ï¼‰ï¼Œæ¡ç”¨æœ€å¾Œä¸€æ¬¡åˆ‡åˆ†")
    return tr_idx, va_idx

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data", type=str, default="data", help="è³‡æ–™æ ¹ç›®éŒ„ï¼ˆå« meta/ æˆ– meta.csvï¼‰")
    ap.add_argument("--out",  type=str, default="data/tabular_baseline.pkl", help="æ¨¡å‹è¼¸å‡ºè·¯å¾‘")
    ap.add_argument("--test_size", type=float, default=0.2)
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    data_dir = Path(args.data)
    out_pkl  = Path(args.out)

    # 1) è®€è³‡æ–™
    df = load_all_meta(data_dir)
    df = df[df["label"].isin(["understood","confused"])].copy()
    if df.empty:
        raise ValueError("è®€åˆ°çš„è³‡æ–™æ²’æœ‰ 'understood' æˆ– 'confused'ã€‚")

    # 2) ç‰¹å¾µèˆ‡æ¨™ç±¤
    feats = pick_features(df)
    if not feats:
        raise ValueError("æ‰¾ä¸åˆ°å¯ç”¨çš„æ•¸å€¼ç‰¹å¾µæ¬„ä½ã€‚")
    X = df[feats].astype(float).values
    y = (df["label"] == "confused").astype(int).values
    groups = derive_groups(df)

    # çµ±è¨ˆ
    print("=== è³‡æ–™çµ±è¨ˆ ===")
    print("æ¨£æœ¬æ•¸ =", len(df), "ï¼›å—æ¸¬è€…æ•¸ =", len(np.unique(groups.astype(str))))
    print("é¡åˆ¥åˆ†ä½ˆï¼ˆ0=understood, 1=confusedï¼‰ï¼š", dict(zip(*np.unique(y, return_counts=True))))
    print("ä½¿ç”¨ç‰¹å¾µï¼š", feats)

    # 3) åˆ†å‰²
    tr_idx, va_idx = robust_split(X, y, groups, test_size=args.test_size, base_seed=args.seed)
    Xtr, Xva, ytr, yva = X[tr_idx], X[va_idx], y[tr_idx], y[va_idx]

    # 4) æ¨™æº–åŒ– + LR
    scaler = StandardScaler().fit(Xtr)
    clf = LogisticRegression(max_iter=500, class_weight="balanced", solver="lbfgs")
    clf.fit(scaler.transform(Xtr), ytr)

    # 5) è©•ä¼°
    yp = clf.predict(scaler.transform(Xva))
    print("\n=== Validation Report ===")
    print(classification_report(yva, yp, target_names=["understood","confused"], digits=3))
    print("F1(macro) =", f1_score(yva, yp, average="macro"))
    print("Confusion matrix:\n", confusion_matrix(yva, yp))

    # 6) å­˜æ¨¡å‹ + æ‘˜è¦
    out_pkl.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump({"scaler": scaler, "clf": clf, "feats": feats}, out_pkl)
    print("\nâœ… å·²å­˜æ¨¡å‹ï¼š", out_pkl.resolve())

    summary = {
        "data_dir": str(data_dir.resolve()),
        "n_samples": int(len(df)),
        "n_subjects": int(len(np.unique(groups.astype(str)))),
        "class_counts": {int(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))},
        "features": feats,
        "test_size": args.test_size,
        "seed": args.seed,
        "model_path": str(out_pkl.resolve())
    }
    (out_pkl.parent / "train_summary.json").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")
    print("ğŸ“ å·²å¯«å…¥è¨“ç·´æ‘˜è¦ï¼š", (out_pkl.parent / "train_summary.json").resolve())

if __name__ == "__main__":
    main()
