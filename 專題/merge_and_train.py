# -*- coding: utf-8 -*-
"""
merge_and_train.py
------------------
åŠŸèƒ½ï¼š
1) æƒæ data/ è³‡æ–™å¤¾åº•ä¸‹ç¬¦åˆæ¨£å¼çš„æª”æ¡ˆï¼ˆé è¨­ï¼šsession_*.csvï¼‰
2) è‡ªå‹•åˆä½µæˆä¸€ä»½ç¸½è¡¨ data/all_features.csv
   - å…è¨±ä¸åŒæª”çš„æ¬„ä½ä¸å®Œå…¨ä¸€è‡´ï¼ˆä¾‹å¦‚ 468 vs 478 é»ï¼‰ï¼šæœƒåšæ¬„ä½è¯é›†ï¼Œç¼ºçš„è£œ 0
   - åƒ…ä¿ç•™ label âˆˆ {0,1} çš„åˆ—ï¼›ä¸Ÿæ£„ç¼º label çš„åˆ—
   - ä¾ time æ’åºã€ç§»é™¤å®Œå…¨é‡è¤‡åˆ—
3) å‘¼å« train_affect_classifier.py é€²è¡Œè¨“ç·´ï¼ˆå¯æŒ‡å®š modelã€tagã€cvï¼‰

ç”¨æ³•ï¼š
    python merge_and_train.py
    python merge_and_train.py --model rf --tag wk2 --cv 5
    python merge_and_train.py --pattern "session_2025*.csv"

å…ˆæ±ºæ¢ä»¶ï¼š
- å·²æœ‰ train_affect_classifier.py èˆ‡å…¶ç›¸ä¾å¥—ä»¶ï¼ˆsklearn, pandas, joblibï¼‰
- ä½ çš„æ”¶é›†è…³æœ¬æœƒæŠŠæª”æ¡ˆæ”¾åœ¨ ./data/ ä¹‹ä¸‹
"""

import os
import glob
import argparse
import subprocess
import sys
import pandas as pd
import numpy as np

def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, default="data",
                    help="è³‡æ–™å¤¾è·¯å¾‘ï¼ˆé è¨­ dataï¼‰")
    ap.add_argument("--pattern", type=str, default="session_*.csv",
                    help="è¦åˆä½µçš„æª”åæ¨£å¼ï¼ˆglobï¼‰ï¼Œé è¨­ session_*.csv")
    ap.add_argument("--output", type=str, default=os.path.join("data", "all_features.csv"),
                    help="è¼¸å‡ºç¸½è¡¨è·¯å¾‘ï¼ˆé è¨­ data/all_features.csvï¼‰")
    ap.add_argument("--model", type=str, default="logreg", choices=["logreg", "rf"],
                    help="è¨“ç·´æ¨¡å‹ç¨®é¡ï¼šlogreg æˆ– rfï¼ˆé è¨­ logregï¼‰")
    ap.add_argument("--tag", type=str, default="merged",
                    help="æ¨¡å‹å­˜æª”ç”¨çš„æ¨™ç±¤ï¼ˆå¯«é€²æª”åï¼Œé è¨­ mergedï¼‰")
    ap.add_argument("--cv", type=int, default=0,
                    help="K æŠ˜äº¤å‰é©—è­‰ï¼ˆ0=ä¸å•Ÿç”¨ï¼›ä¾‹å¦‚ 5 å•Ÿç”¨ 5-foldï¼‰")
    ap.add_argument("--train_script", type=str, default="train_affect_classifier.py",
                    help="è¨“ç·´è…³æœ¬è·¯å¾‘ï¼ˆé è¨­ train_affect_classifier.pyï¼‰")
    ap.add_argument("--keep_source", action="store_true",
                    help="åˆä½µå¾Œä¿ç•™ source æ¬„ï¼ˆä¾†æºæª”åï¼‰ï¼Œé è¨­ä¸ä¿ç•™")
    return ap.parse_args()

def list_input_files(data_dir: str, pattern: str):
    glob_pattern = os.path.join(data_dir, pattern)
    files = sorted(glob.glob(glob_pattern))
    return [f for f in files if os.path.isfile(f)]

def read_one_csv(path: str) -> pd.DataFrame:
    df = pd.read_csv(path, encoding="utf-8")
    df["__source"] = os.path.basename(path)
    return df

def union_concat(dfs):
    """
    æ¬„ä½åšè¯é›†å¾Œåˆä½µï¼›ç¼ºçš„æ¬„ä½è£œ 0ï¼ˆé™¤äº† label/time/source ä¿ç•™åŸæ¨£ï¼‰
    """
    # å…¨éƒ¨æ¬„ä½è¯é›†
    all_cols = set()
    for d in dfs:
        all_cols.update(d.columns)

    # çµ±ä¸€é †åºï¼šå„ªå…ˆ time, label, face_width, face_heightï¼›å† dx_/dy_/dz_ï¼›æœ€å¾Œå…¶ä»–
    base_order = ["time", "label", "face_width", "face_height"]
    dyn_cols = sorted([c for c in all_cols if c.startswith(("dx_", "dy_", "dz_"))],
                      key=lambda x: (x.split("_")[0], int(x.split("_")[1])))
    other_cols = sorted([c for c in all_cols if c not in set(base_order) | set(dyn_cols) | {"__source"}])

    ordered_cols = base_order + dyn_cols + other_cols + ["__source"]

    # é€å€‹è£œé½Šç¼ºæ¬„ä½ä¸¦é‡æ’
    fixed = []
    for d in dfs:
        for col in ordered_cols:
            if col not in d.columns:
                # å°æ–¼æ•¸å€¼ç‰¹å¾µè£œ 0ï¼›å…¶ä»–æ¬„ä½è£œ NaN
                if col.startswith(("dx_","dy_","dz_")) or col in ("face_width","face_height"):
                    d[col] = 0.0
                elif col in ("time","label"):
                    d[col] = np.nan
                else:
                    d[col] = np.nan
        fixed.append(d[ordered_cols])

    out = pd.concat(fixed, axis=0, ignore_index=True)
    return out

def clean_merged(df: pd.DataFrame, keep_source: bool = False) -> pd.DataFrame:
    # åƒ…ä¿ç•™åˆæ³• label
    if "label" in df.columns:
        df = df[df["label"].isin([0,1])]
    else:
        raise SystemExit("âŒ åˆä½µå¾Œæ²’æœ‰ label æ¬„ä½ï¼Œè«‹æª¢æŸ¥ä¾†æº CSVã€‚")

    # time æ¬„ä½è‹¥ç¼ºå¤±å°±è£œæˆç•¶å‰ç´¢å¼•ï¼ˆä¸ç†æƒ³ä½†å¯å·¥ä½œï¼‰
    if "time" not in df.columns:
        df["time"] = np.arange(len(df))
    else:
        # å˜—è©¦è½‰ç‚ºæµ®é»
        df["time"] = pd.to_numeric(df["time"], errors="coerce")

    # ä¾æ™‚é–“æ’åº
    df = df.sort_values("time", ascending=True)

    # ä¸Ÿæ‰å®Œå…¨é‡è¤‡åˆ—ï¼ˆå«ç‰¹å¾µæ¬„ï¼‰
    before = len(df)
    df = df.drop_duplicates()
    after = len(df)
    removed = before - after
    if removed > 0:
        print(f"â„¹ï¸ ç§»é™¤å®Œå…¨é‡è¤‡åˆ—ï¼š{removed} ç­†")

    # æ¸…ç†éæ•¸å€¼æ¬„ä½ï¼ˆé™¤äº† time/label/__sourceï¼‰ï¼Œå…¶é¤˜éæ•¸å€¼æ¬„å˜—è©¦è½‰ 0
    for c in df.columns:
        if c in ("time","label","__source"):
            continue
        if df[c].dtype.kind not in "biufc":  # éæ•¸å€¼
            df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)

    # è‹¥ä¸ä¿ç•™ä¾†æºæ¬„ï¼Œåˆªæ‰
    if not keep_source and "__source" in df.columns:
        df = df.drop(columns=["__source"])

    # é‡è¨­ç´¢å¼•
    return df.reset_index(drop=True)

def save_csv(df: pd.DataFrame, out_path: str):
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    df.to_csv(out_path, index=False, encoding="utf-8")
    print(f"âœ… å·²è¼¸å‡ºåˆä½µæª”ï¼š{out_path}ï¼ˆå…± {len(df)} ç­†ï¼‰")
    if "label" in df.columns:
        uniq, cnt = np.unique(df["label"].astype(int), return_counts=True)
        print("é¡åˆ¥åˆ†ä½ˆï¼š", dict(zip(uniq.tolist(), cnt.tolist())))

def call_train(train_script: str, csv_path: str, model: str, tag: str, cv: int):
    if not os.path.exists(train_script):
        raise SystemExit(f"âŒ æ‰¾ä¸åˆ°è¨“ç·´è…³æœ¬ï¼š{train_script}")

    cmd = [
        sys.executable,           # ä½¿ç”¨ç•¶å‰ Python
        train_script,
        "--csv", csv_path,
        "--model", model,
        "--tag", tag
    ]
    if cv and cv >= 2:
        cmd += ["--cv", str(cv)]

    print("â–¶ é–‹å§‹è¨“ç·´ï¼š", " ".join(cmd))
    # Windows è·¯å¾‘æœ‰ç©ºç™½æ™‚ï¼Œç”¨ list å½¢å¼æœ€å®‰å…¨
    proc = subprocess.run(cmd)
    if proc.returncode != 0:
        raise SystemExit(f"âŒ è¨“ç·´è…³æœ¬å¤±æ•—ï¼ˆexit code {proc.returncode}ï¼‰")

def main():
    args = parse_args()

    # 1) åˆ—å‡ºè¦åˆä½µçš„æª”æ¡ˆ
    files = list_input_files(args.data_dir, args.pattern)
    if not files:
        raise SystemExit(f"âŒ æ‰¾ä¸åˆ°ç¬¦åˆæ¨£å¼çš„æª”æ¡ˆï¼š{os.path.join(args.data_dir, args.pattern)}")

    print("ğŸ” å°‡åˆä½µä»¥ä¸‹æª”æ¡ˆï¼š")
    for f in files:
        print("  -", f)

    # 2) è®€å…¥ä¸¦åˆä½µï¼ˆæ¬„ä½è¯é›†ï¼‰
    dfs = [read_one_csv(p) for p in files]
    merged = union_concat(dfs)

    # 3) æ¸…ç†ï¼ˆä¿ç•™ label=0/1ã€æ’åºã€å»é‡ã€è£œå€¼ï¼‰
    merged = clean_merged(merged, keep_source=args.keep_source)

    # 4) è¼¸å‡º all_features.csv
    save_csv(merged, args.output)

    # 5) å‘¼å«è¨“ç·´
    call_train(args.train_script, args.output, args.model, args.tag, args.cv)

if __name__ == "__main__":
    main()
