# -*- coding: utf-8 -*-
"""
æŠŠåŸæœ¬ YOLO ç”¨çš„å½±åƒè³‡æ–™å¤¾ï¼ˆclass å­è³‡æ–™å¤¾ï¼‰è½‰ç‚º scikit-learn å¯ç”¨çš„ç‰¹å¾µï¼Œ
ä¸¦è¨“ç·´/è©•ä¼°ä¸€å€‹åˆ†é¡å™¨ï¼ˆLogisticRegressionï¼‰ã€‚
æ”¯æ´å…©ç¨®ç‰¹å¾µï¼š
  1) landmarksï¼šç”¨ MediaPipe FaceMesh èƒå–è‡‰éƒ¨å¹¾ä½•ç‰¹å¾µï¼ˆæ¨è–¦ï¼‰
  2) pixels   ï¼šç°éš+ç¸®æ”¾å¾Œæ”¤å¹³æˆå‘é‡ï¼ˆå¿«é€ŸåŸºç·šï¼‰

è³‡æ–™å¤¾æ ¼å¼ï¼š
data/
  understood/*.jpg|png|jpeg
  confused/*.jpg|png|jpeg
"""

import argparse, sys, os, math
from pathlib import Path
import numpy as np
import pandas as pd
import cv2

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline
import joblib

# --------- MediaPipe (åƒ…åœ¨ method=landmarks éœ€è¦) ----------
try:
    import mediapipe as mp
    MP_AVAILABLE = True
except Exception:
    MP_AVAILABLE = False


IMG_EXTS = {".jpg", ".jpeg", ".png", ".bmp", ".tif", ".tiff"}


def iter_images_by_class(data_dir: Path):
    """èµ°è¨ª data_dir ä¸‹é¢çš„æ¯å€‹å­è³‡æ–™å¤¾ï¼Œå­è³‡æ–™å¤¾åè¦–ç‚º labelã€‚"""
    for cls_dir in sorted([p for p in data_dir.iterdir() if p.is_dir()]):
        label = cls_dir.name
        for img_path in cls_dir.rglob("*"):
            if img_path.suffix.lower() in IMG_EXTS:
                yield img_path, label


# =========================
# æ–¹æ³•ä¸€ï¼šåƒç´ æ”¤å¹³ç‰¹å¾µ
# =========================
def extract_pixels_feature(img_bgr, img_size=64):
    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
    gray = cv2.resize(gray, (img_size, img_size), interpolation=cv2.INTER_AREA)
    feat = (gray.astype(np.float32) / 255.0).flatten()
    return feat


# =========================
# æ–¹æ³•äºŒï¼šFaceMesh å¹¾ä½•ç‰¹å¾µ
# =========================
def _dist(p1, p2):
    return float(np.linalg.norm(np.array(p1) - np.array(p2)))

def _safe_ratio(a, b, eps=1e-6):
    return float(a / (b + eps))

def extract_landmarks_feature(img_bgr, face_mesh):
    """å›å‚³ä¸€çµ„æè¿°è‡‰éƒ¨å¹¾ä½•çš„ç‰¹å¾µï¼›åµæ¸¬å¤±æ•—å›å‚³ Noneã€‚"""
    h, w = img_bgr.shape[:2]
    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)

    res = face_mesh.process(rgb)
    if not res.multi_face_landmarks:
        return None

    # å–ç¬¬ä¸€å¼µè‡‰
    lm = res.multi_face_landmarks[0].landmark
    # æŠŠéœ€è¦çš„ 2D åº§æ¨™è½‰æˆåƒç´ åº§æ¨™ï¼ˆx,yï¼‰
    def pt(i):
        return (lm[i].x * w, lm[i].y * h)

    # åƒè€ƒé»ï¼ˆMediaPipe FaceMesh ç´¢å¼•ï¼‰
    # çœ¼è§’ï¼šå·¦çœ¼å¤–å´ 33ã€å³çœ¼å¤–å´ 263ï¼›å·¦çœ¼å…§å´ 133ã€å³çœ¼å…§å´ 362
    # çœ¼ç›å…¶ä»–é»ï¼ˆè¨ˆç®— EAR å¸¸ç”¨ï¼‰ï¼šå·¦ 160,158,144,153ï¼›å³ 387,385,373,380
    L_OUT, L_IN = 33, 133
    R_OUT, R_IN = 263, 362
    L_UP1, L_UP2, L_LO1, L_LO2 = 160, 158, 144, 153
    R_UP1, R_UP2, R_LO1, R_LO2 = 387, 385, 373, 380

    # å˜´è§’ï¼šå·¦ 61ã€å³ 291ï¼›ä¸Šå”‡ 13ã€ä¸‹å”‡ 14ï¼ˆè¿‘ä¼¼ MARï¼‰
    M_LEFT, M_RIGHT, M_UP, M_LOW = 61, 291, 13, 14

    try:
        # EARï¼ˆeye aspect ratioï¼‰ï¼Œè¶Šå°è¶Šåƒé–‰çœ¼
        def ear(left=True):
            if left:
                p1, p2 = pt(L_OUT), pt(L_IN)
                pv1, pv2, pv3, pv4 = pt(L_UP1), pt(L_UP2), pt(L_LO1), pt(L_LO2)
            else:
                p1, p2 = pt(R_OUT), pt(R_IN)
                pv1, pv2, pv3, pv4 = pt(R_UP1), pt(R_UP2), pt(R_LO1), pt(R_LO2)
            # å‚ç›´è·é›¢
            v1 = _dist(pv1, pv3)
            v2 = _dist(pv2, pv4)
            # æ°´å¹³è·é›¢
            hdist = _dist(p1, p2)
            return _safe_ratio(v1 + v2, 2.0 * hdist)

        ear_l = ear(True)
        ear_r = ear(False)

        # MARï¼ˆmouth aspect ratioï¼‰ï¼šä¸Šä¸‹å”‡è· / å˜´è§’è·
        mouth_h = _dist(pt(M_UP), pt(M_LOW))
        mouth_w = _dist(pt(M_LEFT), pt(M_RIGHT))
        mar = _safe_ratio(mouth_h, mouth_w)

        # IPDï¼ˆinterpupillary distanceï¼‰ï¼šå·¦å³çœ¼å¤–å´è§’è· / è‡‰å¯¬è¿‘ä¼¼
        ipd = mouth_w  # ä¹Ÿå¯ç”¨ _dist(pt(L_OUT), pt(R_OUT))
        # äº®åº¦èˆ‡å°æ¯”ï¼ˆç°åº¦å¹³å‡èˆ‡æ¨™æº–å·®ï¼‰
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        mean_intensity = float(gray.mean())
        std_intensity = float(gray.std())

        return np.array([
            ear_l, ear_r, mar, ipd, mean_intensity, std_intensity
        ], dtype=np.float32)
    except Exception:
        return None


def build_features(data_dir: Path, method: str, img_size: int):
    rows = []
    X_list, y_list = [], []

    if method == "landmarks" and not MP_AVAILABLE:
        print("âŒ éœ€è¦ mediapipeï¼Œä½†ç›®å‰å°šæœªå®‰è£æˆ–è¼‰å…¥å¤±æ•—ã€‚è«‹å…ˆï¼špip install mediapipe", file=sys.stderr)
        sys.exit(1)

    face_mesh = None
    if method == "landmarks":
        mp_face = mp.solutions.face_mesh
        face_mesh = mp_face.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5
        )

    for img_path, label in iter_images_by_class(data_dir):
        img = cv2.imread(str(img_path))
        if img is None:
            continue

        if method == "pixels":
            feat = extract_pixels_feature(img, img_size=img_size)
            feat_names = [f"px_{i}" for i in range(feat.size)]
        else:
            feat = extract_landmarks_feature(img, face_mesh)
            feat_names = ["ear_l", "ear_r", "mar", "ipd", "mean_intensity", "std_intensity"]

        if feat is None:
            # è‡‰åµæ¸¬å¤±æ•—æˆ–å…¶ä»–å•é¡Œ
            continue

        X_list.append(feat)
        y_list.append(label)

        row = {"path": str(img_path), "label": label}
        for k, v in zip(feat_names, feat.tolist()):
            row[k] = v
        rows.append(row)

    if face_mesh is not None:
        face_mesh.close()

    if not X_list:
        print("âŒ æ²’æœ‰æˆåŠŸç”¢ç”Ÿä»»ä½•ç‰¹å¾µï¼Œè«‹æª¢æŸ¥è³‡æ–™å¤¾èˆ‡åœ–ç‰‡å…§å®¹ã€‚", file=sys.stderr)
        sys.exit(1)

    X = np.vstack(X_list)
    y = np.array(y_list)
    df = pd.DataFrame(rows)
    return X, y, df


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--data_dir", type=str, required=True, help="è³‡æ–™æ ¹ç›®éŒ„ï¼ˆåº•ä¸‹æ”¾å„é¡åˆ¥å­è³‡æ–™å¤¾ï¼‰")
    ap.add_argument("--method", type=str, default="landmarks", choices=["landmarks", "pixels"],
                    help="ç‰¹å¾µèƒå–æ–¹å¼ï¼šlandmarksï¼ˆæ¨è–¦ï¼‰æˆ– pixelsï¼ˆåŸºç·šï¼‰")
    ap.add_argument("--img_size", type=int, default=64, help="pixels æ–¹æ³•çš„ç¸®æ”¾é‚Šé•·")
    ap.add_argument("--test_size", type=float, default=0.2)
    ap.add_argument("--val_size", type=float, default=0.0, help="ï¼ˆå¯é¸ï¼‰è‹¥æƒ³å†åˆ‡é©—è­‰é›†ï¼Œå¯åœ¨è¨“ç·´é›†å…§å†åˆ‡")
    ap.add_argument("--seed", type=int, default=42)
    args = ap.parse_args()

    data_dir = Path(args.data_dir)
    out_csv = data_dir / "meta.csv"
    out_model = data_dir / "tabular_model.pkl"

    print(f"ğŸ“‚ æƒæè³‡æ–™å¤¾ï¼š{data_dir}")
    print(f"ğŸ”§ ç‰¹å¾µæ–¹æ³•ï¼š{args.method}")

    X, y, df = build_features(data_dir, method=args.method, img_size=args.img_size)

    # å­˜ CSVï¼ˆæ–¹ä¾¿ä½ ä¹‹å¾Œç”¨åˆ¥çš„æ¨¡å‹é‡è¤‡å¯¦é©—ï¼‰
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ å·²è¼¸å‡ºç‰¹å¾µè¡¨ï¼š{out_csv}ï¼ˆ{len(df)} ç­†ï¼‰")

    # åˆ‡è³‡æ–™
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=args.test_size, random_state=args.seed, stratify=y
    )

    # ä½ ä¹Ÿå¯ä»¥åŠ  val_sizeï¼šåœ¨è¨“ç·´é›†å…§å†åˆ‡ä¸€ä»½é©—è­‰é›†ï¼Œé€™è£¡å…ˆçµ¦ç¯„ä¾‹ï¼ˆé è¨­ä¸åˆ‡ï¼‰
    if args.val_size > 0:
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=args.val_size, random_state=args.seed, stratify=y_train
        )
        print(f"ğŸ“¦ Train={len(y_train)}, Val={len(y_val)}, Test={len(y_test)}")
    else:
        print(f"ğŸ“¦ Train={len(y_train)}, Test={len(y_test)}")

    # å»º pipelineï¼šæ¨™æº–åŒ– + é‚è¼¯è¿´æ­¸ï¼ˆä½ å¯ä»¥å¾ˆå¿«æ› clfï¼‰
    clf = Pipeline([
        ("scaler", StandardScaler(with_mean=True, with_std=True)),
        ("lr", LogisticRegression(max_iter=1000, n_jobs=None))
    ])

    print("ğŸš€ é–‹å§‹è¨“ç·´...")
    clf.fit(X_train, y_train)

    # è©•ä¼°
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"\nğŸ¯ Test Accuracy: {acc:.4f}\n")
    print("ğŸ“Š Classification Report:")
    print(classification_report(y_test, y_pred, digits=4))
    print("ğŸ§© Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    # å­˜æ¨¡å‹
    joblib.dump(clf, out_model)
    print(f"\nğŸ’¾ å·²å­˜æ¨¡å‹ï¼š{out_model}")

    print("\nâœ… å®Œæˆï¼ä½ å¯ä»¥ç›´æ¥ç”¨ meta.csv åšæ›´å¤š sklearn å˜—è©¦ï¼Œæˆ–è¼‰å…¥ tabular_model.pkl ç›´æ¥é æ¸¬ã€‚")


if __name__ == "__main__":
    main()
